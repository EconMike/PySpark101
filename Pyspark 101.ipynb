{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a3d851f",
   "metadata": {},
   "source": [
    "# PySpark 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e5e0e",
   "metadata": {},
   "source": [
    "PySpark is the Python API for Apache Spark. It enables you to process massive amounts of data in a relatively short time. \n",
    "This large-scale data processing in a distributed environment using Python. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a31d0592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The modules below allows us to create or support the spark enviroment in Python.  \n",
    "\n",
    "import os\n",
    "import sys\n",
    "import findspark\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "#the module below calls in the garbage collector module python-Garbage collection helps\n",
    "#Apache Spark, which heavily relies on the Java Virtual Machine (JVM) to execute its code. \n",
    "#This module gives tools to automatically free up memory that \n",
    "#is no longer being used by Spark, and it plays a critical role in\n",
    "#ensuring that your Spark session runs smoothly and efficiently\n",
    "\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d02c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#the module below creates the spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"SparkByExamples.com\") \\\n",
    "      .getOrCreate() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd713c3",
   "metadata": {},
   "source": [
    "## PySpark RDD \n",
    "\n",
    "Resilient Distributed Datasets **(RDD)** is one of the fundamental data structures in Spark. Used for handling both structured and unstructured data but lacks any schema. Compared to network and disc sharing, PySpark RDD speeds up in-memory data sharing by 10 to 100 times.\n",
    "\n",
    "**Some Features:**\n",
    "1.  In-Memory \n",
    "2. Lazy Evaluations \n",
    "3. Immutable and Read-only \n",
    "4. Cacheable or Persistence \n",
    "5. Partitioned \n",
    "6. Parallel \n",
    "7. Fault Tolerance \n",
    "8. Location Stickiness  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "764a3fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "#RDD\n",
    "\n",
    "# Create an RDD from a list of numbers\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(numbers)\n",
    "\n",
    "# Perform a transformation on the RDD\n",
    "squared_rdd = rdd.map(lambda x: x * x)\n",
    "\n",
    "# Collect the results into a list\n",
    "result = squared_rdd.collect()\n",
    "\n",
    "# Print the result\n",
    "print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393efad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data= [(\"Dog food\", 25), (\"Cat food\", 30), (\"Dog food\",35),(\"Bear food\",40)]\n",
    "\n",
    "rdd2 =spark.sparkContext.parallelize(data)\n",
    "rdd2.collect()\n",
    "#spark.sparkContext._jvm.System.gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c64a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformations-  operations that creates new RDD from an old one\n",
    "#Tranformations does not take place until an action is called - Lazy operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e9060",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_rdd = rdd2.map(lambda x: (x[0].upper(), x[1]))\n",
    "result= mapped_rdd.collect()\n",
    "\n",
    "print(\"TRANSFORMED DATA:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0130fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter transformation: Filter records where age is greater than 30\n",
    "\n",
    "filtered_rdd= rdd2.filter(lambda x: x[1]>30)\n",
    "filtered_rdd.collect()\n",
    "\n",
    "#sum the cost of food bought for different animals\n",
    "\n",
    "#ReduceByKey transformation: Calculate the total\n",
    "reduced_rdd= rdd2.reduceByKey(lambda x,y: x+y)\n",
    "reduced_rdd.collect()\n",
    "\n",
    "# Sortby transform: Sort the RDD\n",
    "\n",
    "sorted_rdd= reduced_rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "sorted_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0e8328",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Action\n",
    "count = rdd2.count()\n",
    "print(\"The total number of elements in the rdd\", count)\n",
    "\n",
    "\n",
    "#first action: Retrieve the first element of the RDD\n",
    "first_element = rdd2.first()\n",
    "\n",
    "print(\"First elements in the rdd\", first_element)\n",
    "\n",
    "taken_elements= rdd2.take(2)\n",
    "print(\"First 2 elements in the rdd\", taken_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "\n",
    "# read csv, all columns will be of type string\n",
    "df = spark.read.option('header','true').csv('F:\\.....\\PySpark\\heart.csv')\n",
    "# tell pyspark the type of the columns - saves time on large dataset. there are other ways to do this, but that's\n",
    "# my favorite\n",
    "\n",
    "#lets check our data types, its different in spark\n",
    "#Get All column names and it's types\n",
    "for field in df.schema.fields:\n",
    "    print(field.name +\" , \"+str(field.dataType))\n",
    "\n",
    "#for one column\n",
    "print(df.schema[\"Age\"].dataType)\n",
    "\n",
    "#Get All column names from DataFrame\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "schema = 'Age INTEGER, Sex STRING, ChestPainType STRING'\n",
    "df = spark.read.csv('F:\\......\\PySpark\\heart.csv', schema=schema, header=True)\n",
    "#Get All column names and it's types\n",
    "for field in df.schema.fields:\n",
    "    print(field.name +\" , \"+str(field.dataType))\n",
    "\n",
    "# let PySpark infer the schema\n",
    "df = spark.read.csv('F:\\.......\\PySpark\\heart.csv', inferSchema=True, header=True)\n",
    "#Get All column names and it's types\n",
    "for field in df.schema.fields:\n",
    "    print(field.name +\" , \"+str(field.dataType))\n",
    "\n",
    "# replace nulls with other value at reading time\n",
    "df = spark.read.csv('F:\\.......\\PySpark\\heart.csv', nullValue='NA')\n",
    "\n",
    "#print Schema\n",
    "df.printSchema()\n",
    "df.show()\n",
    "df.write.csv(\"F:/....../PySpark/heart_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae9333",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save data\n",
    "\n",
    "df.write.csv('F:\\.....\\PySpark\\heart_save.csv', header = True)\n",
    "\n",
    "df.toPandas().to_csv(\"F:\\......\\PySpark\\heart_save.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6197fd6f",
   "metadata": {},
   "source": [
    "## Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e259285",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "\n",
    "df.show(3)\n",
    "\n",
    "\n",
    "df.collect()\n",
    "# count number of rows\n",
    "df.count()\n",
    "\n",
    "df.take(2)\n",
    "df.printSchema()\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97721828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine DataFrames\n",
    "# Create two DataFrames with sales data \n",
    "sales_data1 = [(\"apple\", 3), (\"banana\", 5), (\"orange\", 2)] \n",
    "sales_data2 = [(\"apple\", 4), (\"banana\", 3), (\"orange\", 6)] \n",
    "\n",
    "df1 = spark.createDataFrame(sales_data1, [\"product\", \"quantity\"]) \n",
    "df2 = spark.createDataFrame(sales_data2, [\"product\", \"quantity\"]) \n",
    "\n",
    "# Merge DataFrames using union \n",
    "merged_df = df1.union(df2) \n",
    "\n",
    "# Show the result \n",
    "merged_df.show()\n",
    "\n",
    "# Merge DataFrames using unionByName \n",
    "merged_df = df1.unionByName(df2) \n",
    "\n",
    "# Show the result \n",
    "merged_df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f54c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a column to a Spark DataFrame\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"sue\", 32),\n",
    "        (\"li\", 3),\n",
    "        (\"bob\", 75),\n",
    "        (\"heo\", 13),\n",
    "    ],\n",
    "    [\"first_name\", \"age\"],\n",
    ")\n",
    "\n",
    "df.show()\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df1 = df.withColumn(\n",
    "    \"life_stage\",\n",
    "    when(col(\"age\") < 13, \"child\")\n",
    "    .when(col(\"age\").between(13, 19), \"teenager\")\n",
    "    .otherwise(\"adult\"),\n",
    ")\n",
    "\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea71572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter a Spark DataFrame\n",
    "\n",
    "df1.where(col(\"life_stage\").isin([\"teenager\", \"adult\"])).show()\n",
    "\n",
    "#Group by aggregation on Spark DataFrame\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df1.select(avg(\"age\")).show()\n",
    "#You can also compute the average age for each life_stage:\n",
    "\n",
    "df1.groupBy(\"life_stage\").avg().show()\n",
    "\n",
    "\n",
    "df.select(\"firstname\",\"middlename\").show(2)\n",
    "\n",
    "\n",
    "# show parts of the table\n",
    "df.select('Age').show(3)\n",
    "df.select(['Age','Sex']).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run SQL queries in PySpark\n",
    "# Registering a table\n",
    "#df.registerTempTable(\"df\")\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "query_df = spark.sql(\"select * from df\").show(3)\n",
    "\n",
    "#Run a query that returns the teenagers:\n",
    "\n",
    "spark.sql(\"select * from df where first_name='sue'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89785d0",
   "metadata": {},
   "source": [
    "## Spark Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modules for machine learning in pyspark\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128fda3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mport data\n",
    "data = spark.read.csv(SparkFiles.get(\"F:\\........\\PySpark\\BostonHousing.csv\"), header=True, inferSchema=True)\n",
    "data.show(5)\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248db053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assemble the data\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\",  \"LSTAT\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "data = assembler.transform(data)\n",
    "final_data = data.select(\"features\", \"MEDV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc61b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"MEDV\", predictionCol=\"predicted_medv\")\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "\n",
    "\n",
    "predictions = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15e719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaulation metrics\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"MEDV\", predictionCol=\"predicted_medv\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data: {:.3f}\".format(rmse))\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"MEDV\", predictionCol=\"predicted_medv\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "print(\"R-squared (R2) on test data: {:.3f}\".format(r2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1e276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#shut down spark session\n",
    "spark.stop()\n",
    "\n",
    "#problems with memory- clear cache\n",
    "spark.catalog.clearCache()  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
